name: pretrained
probe:
  hidden_dim: 243
######################### dataset parameters #########################
# dataset containing images and original targets
dataset: dsprites # dsprites / shapes3d / mpi3d
dataset_path: ./data/disent_indommain/
cached_representations: True
# dataset containing extracted reprentations and processed targets
representation_dataset_path: ./results/2022Oct19-142016_dsprites_hsicbetavae_3s/0000_default/0/dsprites_composition_vae_pretrained_dataset
resnet_representation_dataset_path: ../checkpoints/resnet18/mpi3d_resnet18_pretrained_dataset
vae_representation_dataset_path: /home/wliang/Github/loss_capacity/src/results/2022Oct19-142016_dsprites_hsicbetavae_3s/

train_run_id: 0





num_workers: 2
variant: 'composition'
# batch_size: 64
# epochs: 100
# lr: 0.001
# gamma: 0.7 # Learning rate step gamma (default: 0.7)
dry-run: False
seed: 1
log_interval: 10 # log every N batches
save_model: True
######################### model parameters #########################
# model used for obtaining the representations chose from 
# [raw_data/ noisy_labels / random_linear_mix_labels / noisy_uniform_mix_labels / resnet18]
model_type: vae
supervision: False
pretrained: True
noise_std: 0.0 
save_representation_datasets: True
model:
  lr: 0.001
  epochs: 100
  batch_size: 64
  gamma: 0.1 # Learning rate step gamma (default: 0.7)
  optim_steps: [70, 90]
######################### probe parameters #########################

probe:
  type: MLP_reg_cl_ind # MLP_reg / MLP_reg_cl / tree_ens_rf / tree_ens_hgb 
  data_fraction: 1.0
  max_leaf_nodes: None
  max_depth: 50
  num_trees: 100
  hidden_layers: 2
  hidden_multiplier: 128 # size of the hidden layer (multiplier x num_factors) (default: 16)
  rf_lr: 0.1
  lr: 0.0002
  weight_decay: 0.0
  use_norm: False
  use_dropout: False
  epochs: 50
  batch_size: 128
  gamma: 0.1
  optim_steps: [70, 90]
  noise_std_mult: 1.0
  cross_ent_mult: 1.0


vae:
  model_params:
    name: 'HsicBetaVAE'
    in_channels: 3
    latent_dim: 10
    loss_type: 'H'
    beta: 4.
    recons_type: 'bce'
    model_type: 'smallhsicbeta' # small / big / smallhsicbeta /bighsicbeta
    
  data_params:
    data_path: "Data/"
    train_batch_size: 64
    val_batch_size:  64
    patch_size: 64
    num_workers: 4


  exp_params:
    LR: 0.002
    weight_decay: 0.0
    scheduler_gamma: 0.95
    kld_weight: 1.0 #0.00025
    manual_seed: 1265
    alpha: 0.0
  trainer_params:
    gpus: [0]
    max_epochs: 30

  logging_params:
    save_dir: "logs/"
    # name: 'BetaVAE'
    name: 'VAE'

