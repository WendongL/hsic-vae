name: pretrained

######################### dataset parameters #########################
# dataset containing images and original targets
dataset: dsprites # dsprites / shapes3d / mpi3d
dataset_path: ./data/disent_indommain/
# cached_representations: False
# # dataset containing extracted reprentations and processed targets
# representation_dataset_path: ./
# resnet_representation_dataset_path: ../checkpoints/resnet18/dsprites_resnet18_pretrained_dataset
# vae_representation_dataset_path: /home/anicolicioiu/data/models/results_liftoff_loss_capacity_discrete_and_c_good/2022May17-170014_unsup_vae/
# vae_representation_dataset_path: /home/anicolicioiu/data/models/results_liftoff_loss_capacity_discrete_and_c_good/2022May19-125638_unsup_vae/

num_workers: 2

batch_size: 64
epochs: 100
lr: 0.001
gamma: 0.7 # Learning rate step gamma (default: 0.7)
dry-run: False
seed: 1
log_interval: 10 # log every N batches
save_model: True
######################### model parameters #########################
# model used for obtaining the representations chose from 
# [raw_data/ noisy_labels / random_linear_mix_labels / noisy_uniform_mix_labels / resnet18]
model_type: vae 
supervision: False
pretrained: True
noise_std: 0.0 
save_representation_datasets: True
result_path: "./results/2022Sep08-214017_unsup_vae_dsprites_hsicbetavae"
seed_folder: 0
job_folder: '0000'
score_name: 'beta'

hsic:
  xlatent: True
  ylatent: True
  div_subsample: 1000
  num_sample_reparam: 10
  sigma_hsic: [0.001, 0.01, 0.1, 1, 10, 100, 1000] #[0.1, 1, 10, 100, 1000, 10000, 100000]
  choose_dataloader: 'test'
  comp: 'rand'

model:
  lr: 0.001
  epochs: 100
  batch_size: 64
  gamma: 0.1 # Learning rate step gamma (default: 0.7)
  optim_steps: [70, 90]
# ######################### probe parameters #########################

probe:
  type: tree_ens_xgb # MLP_reg / MLP_reg_cl / tree_ens_rf / tree_ens_hgb 
  data_fraction: 1.0
  max_leaf_nodes: None
  max_depth: 50
  num_trees: 100
  hidden_layers: 2
  hidden_multiplier: 128 # size of the hidden layer (multiplier x num_factors) (default: 16)
  rf_lr: 0.1
  lr: 0.0004
  epochs: 100
  batch_size: 128
  gamma: 0.1
  optim_steps: [70, 90]
  noise_std_mult: 1.0
  cross_ent_mult: 1.0
  num_train: 1000
  num_eval: 1000
  num_variance_estimate: 500

vae:
  model_params:
    name: 'HsicBetaVAE'
    in_channels: 1
    latent_dim: 10
    loss_type: 'H'
    beta: 1.
    recons_type: 'bce'
    model_type: 'small' # small / big

  data_params:
    data_path: "Data/"
    train_batch_size: 64
    val_batch_size:  64
    patch_size: 64
    num_workers: 4


  exp_params:
    LR: 0.002
    weight_decay: 0.0
    scheduler_gamma: 0.95
    kld_weight: 1.0 #0.00025
    manual_seed: 1265
    hsic_every_epoch: True
    s_x: 10
    s_y: 10
    num_samples_hisc: 512
    hsic_reg_version: 'v1'


  trainer_params:
    gpus: [0]
    max_epochs: 10

  logging_params:
    save_dir: "logs/"
    # name: 'BetaVAE'
    name: 'VAE'

